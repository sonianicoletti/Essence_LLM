{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nocDLAYiyXrf"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install evaluate\n",
        "!pip install bert_score\n",
        "!pip install ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate BERTScore"
      ],
      "metadata": {
        "id": "3p7Z6bel9ffV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "import pandas as pd\n",
        "\n",
        "# Load BERTScore\n",
        "bertscore = load(\"bertscore\")\n",
        "\n",
        "# Load the Excel file into a pandas DataFrame\n",
        "excel_file = 'dataset.xlsx'  # Path to your dataset Excel file\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Ensure the DataFrame has the required columns\n",
        "required_columns = ['question', 'essence_coach', 'chatgpt_4o', 'ground_truth']\n",
        "if not all(col in df.columns for col in required_columns):\n",
        "    raise ValueError(f\"The Excel file must contain the following columns: {required_columns}\")\n",
        "\n",
        "# Extract data for processing\n",
        "questions = df['question'].tolist()\n",
        "question_type = df['question_type'].tolist()\n",
        "predictions_coach = df['essence_coach'].tolist()\n",
        "predictions_gpt = df['chatgpt_4o'].tolist()\n",
        "references = df['ground_truth'].tolist()\n",
        "\n",
        "# Compute BERTScore for Essence Coach\n",
        "results_coach = bertscore.compute(predictions=predictions_coach, references=references, lang=\"en\")\n",
        "\n",
        "# Compute BERTScore for ChatGPT\n",
        "results_gpt = bertscore.compute(predictions=predictions_gpt, references=references, lang=\"en\")\n",
        "\n",
        "# Combine results with questions into a single DataFrame\n",
        "df_score = pd.DataFrame({\n",
        "    'question': questions,\n",
        "    'question_type': question_type,\n",
        "    'precision_coach': results_coach['precision'],\n",
        "    'recall_coach': results_coach['recall'],\n",
        "    'f1_coach': results_coach['f1'],\n",
        "    'precision_gpt': results_gpt['precision'],\n",
        "    'recall_gpt': results_gpt['recall'],\n",
        "    'f1_gpt': results_gpt['f1'],\n",
        "})\n",
        "\n",
        "# Save the evaluation scores to a CSV file\n",
        "df_score.to_csv('scores_bert.csv', index=False)\n",
        "\n",
        "print(\"Evaluation scores saved to 'scores_bert.csv'\")\n"
      ],
      "metadata": {
        "id": "un7rtdKoycI5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Averages BERTScore"
      ],
      "metadata": {
        "id": "LP43Ss5c9jlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate averages for all rows\n",
        "overall_avg = df_score[['precision_coach', 'recall_coach', 'f1_coach',\n",
        "                        'precision_gpt', 'recall_gpt', 'f1_gpt']].mean()\n",
        "\n",
        "# Filter and calculate averages for specific question types\n",
        "info_avg = df_score[df_score['question_type'] == 'information'][['precision_coach', 'recall_coach', 'f1_coach',\n",
        "                                                                 'precision_gpt', 'recall_gpt', 'f1_gpt']].mean()\n",
        "\n",
        "decision_avg = df_score[df_score['question_type'] == 'decision-making'][['precision_coach', 'recall_coach', 'f1_coach',\n",
        "                                                                         'precision_gpt', 'recall_gpt', 'f1_gpt']].mean()\n",
        "\n",
        "translation_avg = df_score[df_score['question_type'] == 'translation'][['precision_coach', 'recall_coach', 'f1_coach',\n",
        "                                                                        'precision_gpt', 'recall_gpt', 'f1_gpt']].mean()\n",
        "\n",
        "# Combine all averages into a single DataFrame\n",
        "avg_data = pd.DataFrame({\n",
        "    'Metric': ['precision_coach', 'recall_coach', 'f1_coach', 'precision_gpt', 'recall_gpt', 'f1_gpt'],\n",
        "    'Overall': overall_avg.values,\n",
        "    'Information': info_avg.values,\n",
        "    'Decision-Making': decision_avg.values,\n",
        "    'Translation': translation_avg.values,\n",
        "})\n",
        "\n",
        "# Save the averages to a CSV file\n",
        "avg_data.to_csv('scores_bert_avg.csv', index=False)\n",
        "\n",
        "print(\"Average scores saved to 'scores_bert_avg.csv'\")\n"
      ],
      "metadata": {
        "id": "4nMeQ95zCDPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Averages Human Evaluation"
      ],
      "metadata": {
        "id": "Taws-H3M9nES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file into a pandas DataFrame\n",
        "excel_file = 'dataset.xlsx'  # Path to your dataset Excel file\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Ensure the DataFrame has the required columns\n",
        "required_columns = [\n",
        "    'relevance_coach', 'accuracy_coach', 'completeness_coach',\n",
        "    'relevance_gpt', 'accuracy_gpt', 'completeness_gpt', 'question_type'\n",
        "]\n",
        "if not all(col in df.columns for col in required_columns):\n",
        "    raise ValueError(f\"The Excel file must contain the following columns: {required_columns}\")\n",
        "\n",
        "# Compute the overall averages\n",
        "overall_avg = df[[\n",
        "    'relevance_coach', 'accuracy_coach', 'completeness_coach',\n",
        "    'relevance_gpt', 'accuracy_gpt', 'completeness_gpt'\n",
        "]].mean()\n",
        "\n",
        "# Compute averages for each question_type\n",
        "grouped_avg = df.groupby('question_type')[[\n",
        "    'relevance_coach', 'accuracy_coach', 'completeness_coach',\n",
        "    'relevance_gpt', 'accuracy_gpt', 'completeness_gpt'\n",
        "]].mean()\n",
        "\n",
        "# Combine overall and grouped averages into a single DataFrame\n",
        "overall_avg_df = pd.DataFrame(overall_avg, columns=['Overall']).T  # Convert to row\n",
        "grouped_avg_df = grouped_avg.reset_index()  # Reset index for grouped averages\n",
        "\n",
        "# Concatenate overall and grouped averages\n",
        "combined_df = pd.concat([overall_avg_df, grouped_avg_df], ignore_index=False)\n",
        "\n",
        "# Save the combined averages to a CSV file\n",
        "combined_df.to_csv('scores_human_avg.csv', index=False)\n",
        "\n",
        "print(\"Averages saved to 'scores_human_avg.csv'\")\n"
      ],
      "metadata": {
        "id": "XPQZfQr0EDt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table images"
      ],
      "metadata": {
        "id": "EHaKQExM9Yj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file = 'scores_bert_avg.csv'  # Replace with your file path\n",
        "df_avg = pd.read_csv(csv_file)\n",
        "\n",
        "# Round values to 3 decimal places\n",
        "df_avg = df_avg.round(3)\n",
        "\n",
        "# Transpose the DataFrame to invert rows and columns\n",
        "df_avg_transposed = df_avg.set_index('Metric').T.reset_index()\n",
        "df_avg_transposed.rename(columns={'index': 'Category'}, inplace=True)\n",
        "\n",
        "# Create a table using Matplotlib\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust size as needed\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Create the table\n",
        "table = ax.table(cellText=df_avg_transposed.values,\n",
        "                 colLabels=df_avg_transposed.columns,\n",
        "                 cellLoc='center',\n",
        "                 loc='center')\n",
        "\n",
        "# Style the table\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.auto_set_column_width(col=list(range(len(df_avg_transposed.columns))))\n",
        "\n",
        "# Save the table as an image\n",
        "output_image = 'scores_bert_avg_table_inverted.png'  # File name for the image\n",
        "plt.savefig(output_image, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Table saved as an image: {output_image}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9u3-EFixGS3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file = 'scores_human_avg.csv'  # Replace with your file path\n",
        "df_human_avg = pd.read_csv(csv_file)\n",
        "\n",
        "# Round values to 3 decimals for better presentation\n",
        "df_human_avg = df_human_avg.round(3)\n",
        "\n",
        "# Ensure the required columns are present\n",
        "required_columns = ['relevance_coach', 'accuracy_coach', 'completeness_coach',\n",
        "                    'relevance_gpt', 'accuracy_gpt', 'completeness_gpt', 'question_type']\n",
        "\n",
        "if not all(col in df_human_avg.columns for col in required_columns):\n",
        "    raise ValueError(f\"The CSV file must contain the following columns: {required_columns}\")\n",
        "\n",
        "# Extract rows for overall and specific question types\n",
        "categories = ['overall', 'information', 'decision-making', 'translation']\n",
        "averages = []\n",
        "\n",
        "# Compute overall average for all numerical columns\n",
        "overall_avg = df_human_avg[required_columns[:-1]].mean().tolist()\n",
        "averages.append(['overall'] + overall_avg)\n",
        "\n",
        "# Compute averages for each question type\n",
        "for category in categories[1:]:\n",
        "    filtered = df_human_avg[df_human_avg['question_type'] == category]\n",
        "    category_avg = filtered[required_columns[:-1]].mean().tolist()\n",
        "    averages.append([category] + category_avg)\n",
        "\n",
        "# Create a new DataFrame for the table\n",
        "table_df = pd.DataFrame(averages, columns=['Category'] + required_columns[:-1])\n",
        "\n",
        "# Create the table using Matplotlib\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust size as needed\n",
        "ax.axis('tight')\n",
        "ax.axis('off')\n",
        "\n",
        "# Create the table\n",
        "table = ax.table(cellText=table_df.values,\n",
        "                 colLabels=table_df.columns,\n",
        "                 cellLoc='center',\n",
        "                 loc='center')\n",
        "\n",
        "# Style the table\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.auto_set_column_width(col=list(range(len(table_df.columns))))\n",
        "\n",
        "# Save the table as an image\n",
        "output_image = 'scores_human_avg_table.png'  # File name for the image\n",
        "plt.savefig(output_image, dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Table saved as an image: {output_image}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9esOi33XK7JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bar plots"
      ],
      "metadata": {
        "id": "QPVv1Fy49TzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file = 'scores_human_avg.csv'  # Replace with your file path\n",
        "df_human_avg = pd.read_csv(csv_file)\n",
        "\n",
        "# Ensure the required columns are present\n",
        "required_columns = ['relevance_coach', 'accuracy_coach', 'completeness_coach',\n",
        "                    'relevance_gpt', 'accuracy_gpt', 'completeness_gpt', 'question_type']\n",
        "\n",
        "if not all(col in df_human_avg.columns for col in required_columns if col != 'question_type'):\n",
        "    raise ValueError(f\"The CSV file must contain the following columns: {required_columns}\")\n",
        "\n",
        "# Manually set the `question_type` for the first row if missing\n",
        "if pd.isna(df_human_avg.loc[0, 'question_type']):\n",
        "    df_human_avg.loc[0, 'question_type'] = 'Overall'\n",
        "\n",
        "# Define metrics\n",
        "metrics = ['relevance', 'accuracy', 'completeness']\n",
        "\n",
        "# Define question types\n",
        "question_types = ['overall', 'information', 'decision-making', 'translation']\n",
        "\n",
        "# Create a bar plot for each question type\n",
        "for qtype in question_types:\n",
        "    # Filter the data for the current question type\n",
        "    row = df_human_avg[df_human_avg['question_type'].str.lower() == qtype]\n",
        "\n",
        "    if not row.empty:\n",
        "        coach_scores = row[[f\"{metric}_coach\" for metric in metrics]].values.flatten()\n",
        "        gpt_scores = row[[f\"{metric}_gpt\" for metric in metrics]].values.flatten()\n",
        "\n",
        "        # Create the bar plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        width = 0.35  # Bar width\n",
        "        x = np.arange(len(metrics))  # Metric positions\n",
        "\n",
        "        # Plot the bars for Essence Coach and GPT-4o\n",
        "        ax.bar(x - width/2, coach_scores, width, label='Essence Coach', alpha=0.6)\n",
        "        ax.bar(x + width/2, gpt_scores, width, label='GPT-4o', alpha=0.6)\n",
        "\n",
        "        # Set plot labels and title\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(['Relevance', 'Accuracy', 'Completeness'], fontsize=12)\n",
        "        ax.set_ylabel('Scores', fontsize=12)\n",
        "        ax.set_title(f'{qtype.capitalize()} Scores Comparison', fontsize=14)\n",
        "        ax.legend(fontsize=10)\n",
        "\n",
        "        # Save and show the plot\n",
        "        output_image = f'scores_comparison_{qtype}.png'\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(output_image, dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Bar plot for {qtype.capitalize()} saved as an image: {output_image}\")\n",
        "    else:\n",
        "        print(f\"No data found for question type '{qtype}'. It will be skipped.\")\n"
      ],
      "metadata": {
        "id": "5ggC1dup9RrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print all contexts"
      ],
      "metadata": {
        "id": "hmIsLvTCYJky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# File path to the data.jsonl file\n",
        "file_path = \"data.jsonl\"\n",
        "\n",
        "# Read and process the JSONL file\n",
        "with open(file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        # Parse the JSON object\n",
        "        record = json.loads(line)\n",
        "\n",
        "        # Extract the question and contexts\n",
        "        question = record.get(\"user_question\", \"No question found\")\n",
        "        contexts = record.get(\"context\", [])\n",
        "\n",
        "        # Print the question\n",
        "        print(\"User Question:\")\n",
        "        print(question)\n",
        "        print(\"\\nContexts:\")\n",
        "\n",
        "        # Print each context clearly separated\n",
        "        for i, context in enumerate(contexts, start=1):\n",
        "            print(f\"Context {i}:\")\n",
        "            print(context)\n",
        "            print(\"-\" * 40)\n",
        "        print(\"=\" * 80)  # Separator for each record"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lQ83MMn9YNPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Retrieved Contexts"
      ],
      "metadata": {
        "id": "5cUffDu82HiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the file\n",
        "excel_file = 'scores_retrieval.xlsx'\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Ensure required columns exist\n",
        "required_columns = ['k', 'relevant_results', 'total_relevant_results',\n",
        "                    'pos_rel_doc_1', 'pos_rel_doc_2', 'pos_rel_doc_3', 'pos_rel_doc_4']\n",
        "if not all(col in df.columns for col in required_columns):\n",
        "    raise ValueError(f\"The CSV file must contain the following columns: {required_columns}\")\n",
        "\n",
        "# Initialize metrics\n",
        "precision_at_k = []\n",
        "recall_at_k = []\n",
        "reciprocal_ranks = []\n",
        "average_precisions = []\n",
        "\n",
        "# Process each question\n",
        "for _, row in df.iterrows():\n",
        "    k = row['k']\n",
        "    relevant_results = row['relevant_results']\n",
        "    total_relevant = row['total_relevant_results']\n",
        "    positions = [row[f'pos_rel_doc_{i+1}'] for i in range(4) if not pd.isna(row[f'pos_rel_doc_{i+1}'])]\n",
        "\n",
        "    print(\"POSTIONS\")\n",
        "    print(positions)\n",
        "\n",
        "    # Precision@K\n",
        "    precision = relevant_results / k if k > 0 else 0\n",
        "    precision_at_k.append(precision)\n",
        "\n",
        "    # Recall@K\n",
        "    recall = relevant_results / total_relevant if total_relevant > 0 else 0\n",
        "    recall_at_k.append(recall)\n",
        "\n",
        "    # Mean Reciprocal Rank (MRR)\n",
        "    reciprocal_rank = 0\n",
        "    if positions:\n",
        "        reciprocal_rank = 1 / positions[0]\n",
        "    reciprocal_ranks.append(reciprocal_rank)\n",
        "\n",
        "    # Mean Average Precision (MAP)\n",
        "    if positions:\n",
        "        positions_sorted = sorted(positions)\n",
        "        precision_scores = []\n",
        "        for i, pos in enumerate(positions_sorted, start=1):\n",
        "            if pos <= k:\n",
        "                precision_scores.append(i / pos)\n",
        "        average_precision = sum(precision_scores) / len(positions_sorted) if positions_sorted else 0\n",
        "    else:\n",
        "        average_precision = 0\n",
        "    average_precisions.append(average_precision)\n",
        "\n",
        "# Compute averages for the metrics\n",
        "mean_precision_at_k = sum(precision_at_k) / len(precision_at_k) if precision_at_k else 0\n",
        "mean_recall_at_k = sum(recall_at_k) / len(recall_at_k) if recall_at_k else 0\n",
        "mean_reciprocal_rank = sum(reciprocal_ranks) / len(reciprocal_ranks) if reciprocal_ranks else 0\n",
        "mean_average_precision = sum(average_precisions) / len(average_precisions) if average_precisions else 0\n",
        "\n",
        "# Print the results\n",
        "print(f\"Precision@K: {mean_precision_at_k:.3f}\")\n",
        "print(f\"Recall@K: {mean_recall_at_k:.3f}\")\n",
        "print(f\"Mean Reciprocal Rank (MRR): {mean_reciprocal_rank:.3f}\")\n",
        "print(f\"Mean Average Precision (MAP): {mean_average_precision:.3f}\")\n"
      ],
      "metadata": {
        "id": "OQM6-KyBz_H2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}